name: _benchmarks-bencher (Reusable workflow with Bencher.dev integration)

on:
  workflow_call:
    inputs:
      benchmark-project:
        description: Path to the benchmark project to run benchmarks against (cannot be null or empty string)
        type: string
        required: true

      runner-os:
        description: Runner OS (cannot be null or empty string, default ubuntu-latest)
        type: string
        default: ubuntu-latest

      dotnet-version:
        description: Version of .NET SDK to use (cannot be null or empty string, default 10.0.x)
        type: string
        default: 10.0.x

      configuration:
        description: The type of build to produce, e.g. Release vs Debug (cannot be null or empty string, default Release)
        type: string
        default: Release

      preprocessor-symbols:
        description: Preprocessor symbols to pass to the compiler
        type: string
        default: ""

      minver-tag-prefix:
        description: MinVer tag prefix to use for version calculation. Cannot be null or empty!
        type: string
        default: v

      minver-prerelease-id:
        description: MinVer default pre-release identifiers to use for version calculation. Cannot be null or empty!
        type: string
        default: preview.0

      max-regression-pct:
        description: Maximum acceptable performance regression percentage (used for Bencher threshold)
        type: number
        default: 20 # percent

    secrets:
      BENCHER_API_TOKEN:
        description: Bencher.dev API token for authentication
        required: true

permissions:
  contents: read
  pull-requests: write # Required for Bencher to comment on PRs
  checks: write # Required for Bencher to create GitHub Checks

env:
  ARTIFACTS_DIR: ./BenchmarkArtifacts
  DOTNET_NOLOGO: true
  DOTNET_CLI_TELEMETRY_OPTOUT: true
  VERBOSE: ${{ vars.VERBOSE || false }}

jobs:
  benchmarks:
    name: Benchmarks (${{ inputs.runner-os }})
    permissions:
      checks: write # Required for Bencher to create GitHub Checks
    runs-on: ${{ inputs.runner-os }}
    steps:
      # Install Bencher CLI (install early so if it fails, we fail fast before doing extra work)
      - name: Compute Bencher cache week key
        id: cache-date
        run: echo "week=$(date +%Y-W%V)" >> "$GITHUB_OUTPUT"
        shell: bash

      - name: Cache Bencher CLI
        id: cache-bencher
        uses: actions/cache@v5
        with:
          path: ~/.cargo/bin/bencher
          key: bencher-cli-${{ runner.os }}-${{ steps.cache-date.outputs.week }}
          restore-keys: |
            bencher-cli-${{ runner.os }}-latest-

      - name: Install Bencher CLI
        if: steps.cache-bencher.outputs.cache-hit != 'true'
        uses: bencherdev/bencher@v0.5.10

      # Clone the repository with full Git history
      - name: Checkout
        uses: actions/checkout@v6
        with:
          fetch-depth: 0
          filter: tree:0

      - name: Setup DevOps environment variables
        uses: vmelamed/vm2.DevOps/.github/actions/setup-env@main

      - name: Setup .NET and the dependencies cache
        uses: vmelamed/vm2.DevOps/.github/actions/cache-dependencies@main
        with:
          dotnet-version: ${{ inputs.dotnet-version }}
          runner-os: ${{ inputs.runner-os }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          github-actor: ${{ github.actor }}

      # Retrieve compiled binaries from the build job
      - name: Restore build artifacts from cache
        uses: actions/cache/restore@v5
        with:
          key: build-artifacts-${{ runner.os }}-${{ github.sha }}-${{ inputs.configuration }}-${{ github.run_id }}
          fail-on-cache-miss: true
          path: |
            **/bin/${{ inputs.configuration }}
            **/obj

      # Run benchmarks using BenchmarkDotNet and export JSON results
      - name: Run BenchmarkDotNet benchmarks
        id: run-benchmarks
        shell: bash
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}
          GITHUB_TOKEN: ${{ github.token }}
        run: |
          source "$DEVOPS_LIB_DIR/gh_core.sh"

          trace "Running benchmarks for project: ${{ inputs.benchmark-project }} with configuration: ${{ inputs.configuration }} on runner OS: ${{ inputs.runner-os }}"

          run-benchmarks.sh ${{ inputs.benchmark-project }} \
              --configuration "${{ inputs.configuration }}" \
              --define "${{ inputs.preprocessor-symbols }}" \
              --minver-tag-prefix "${{ inputs.minver-tag-prefix }}" \
              --minver-prerelease-id "${{ inputs.minver-prerelease-id }}" \
              --artifacts "${{ env.ARTIFACTS_DIR }}"

          # Calculate thresholds as decimals (e.g., 10% -> LOWER_THRESHOLD=0.90, 10% -> UPPER_THRESHOLD=0.10)
          LOWER_THRESHOLD=$(echo "scale=4; 1 - ${{ inputs.max-regression-pct }} / 100" | bc)
          UPPER_THRESHOLD=$(echo "scale=4; ${{ inputs.max-regression-pct }} / 100" | bc)

          # Compute Bencher project slug from repository name, e.g., vmelamed/vm2.DevOps -> vm2-devops
          REPO_NAME="${{ github.repository }}"
          BENCHER_SLUG=$(echo "${REPO_NAME#*/}" | tr '[:upper:]' '[:lower:]' | tr '.' '-')

          # Build bencher run command
          BENCHER_ARGS=(
              --project "$BENCHER_SLUG"
              --token "$BENCHER_API_TOKEN"
              --github-actions "$GITHUB_TOKEN"
              --iter 1
              --fold median
              --branch "${{ github.ref_name }}"
              --testbed "${{ inputs.runner-os }}"
              --threshold-measure latency
              --threshold-test percentage
              --threshold-max-sample-size 64
              --threshold-lower-boundary _
              --threshold-upper-boundary "$UPPER_THRESHOLD"
              --threshold-measure throughput
              --threshold-test percentage
              --threshold-max-sample-size 64
              --threshold-lower-boundary "$LOWER_THRESHOLD"
              --threshold-upper-boundary _
              --thresholds-reset
              --err
              --adapter c_sharp_dot_net
          )

          trace "Looking for benchmark result files matching: ${{ env.ARTIFACTS_DIR }}/results/*-report.json:"
          trace "$(ls -l ${{ env.ARTIFACTS_DIR }}/results/*-report.json)"

          # Add all benchmark result files
          for file in ${{ env.ARTIFACTS_DIR }}/results/*-report.json; do
              if [[ -s "$file" ]]; then
                  trace "Adding benchmark result file: $file"
                  BENCHER_ARGS+=(--file "$file")
              fi
          done

          trace "Running command: bencher run ${BENCHER_ARGS[*]}"
          if bencher_output=$(bencher run "${BENCHER_ARGS[@]}"); then
              info "Upload to Bencher succeeded."
          else
              error "Upload to Bencher failed."
          fi
          trace "$bencher_output" | to_trace_out

      # Upload benchmark results as workflow artifacts for manual inspection
      - name: Upload benchmark results artifact
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: benchmark-results-${{ inputs.runner-os }}
          path: ${{ env.ARTIFACTS_DIR }}
          if-no-files-found: warn
          overwrite: true

      # Add summary to job
      - name: Add Bencher results to summary
        if: always()
        shell: bash
        run: |
          REPO_NAME="${{ github.repository }}"
          BENCHER_SLUG=$(echo "${REPO_NAME#*/}" | tr '[:upper:]' '[:lower:]' | tr '.' '-')

          {
              echo "### ðŸ“Š Benchmark Results"
              echo ""
              echo "Project: **${{ github.repository }}**"
              echo "Branch: **${{ github.ref_name }}**"
              echo "Testbed: **${{ inputs.runner-os }}**"
              echo "Threshold: **Â±${{ inputs.max-regression-pct }}%**"
              echo ""
              echo "ðŸŒ [View detailed results on Bencher](https://bencher.dev/console/projects/${BENCHER_SLUG})"
          } >> $GITHUB_STEP_SUMMARY
