name: _benchmarks-bencher (Reusable workflow with Bencher.dev integration)

on:
  workflow_call:
    inputs:
      benchmark-project:
        description: Path to the benchmark project to run benchmarks against (cannot be null or empty string)
        type: string
        required: true

      runner-os:
        description: Runner OS (cannot be null or empty string, default ubuntu-latest)
        type: string
        default: ubuntu-latest

      dotnet-version:
        description: Version of .NET SDK to use (cannot be null or empty string, default 10.0.x)
        type: string
        default: 10.0.x

      configuration:
        description: The type of build to produce, e.g. Release vs Debug (cannot be null or empty string, default Release)
        type: string
        default: Release

      preprocessor-symbols:
        description: Preprocessor symbols to pass to the compiler
        type: string
        default: ""

      minver-tag-prefix:
        description: MinVer tag prefix to use for version calculation. Cannot be null or empty!
        type: string
        default: v

      minver-prerelease-id:
        description: MinVer default pre-release identifiers to use for version calculation. Cannot be null or empty!
        type: string
        default: preview.0

      max-regression-pct:
        description: Maximum acceptable performance regression percentage (used for Bencher threshold)
        type: number
        default: 20 # percent

    secrets:
      BENCHER_API_TOKEN:
        description: Bencher.dev API token for authentication
        required: true

permissions:
  contents: read
  pull-requests: write # Required for Bencher to comment on PRs
  checks: write # Required for Bencher to create GitHub Checks

env:
  ARTIFACTS_DIR: ./BenchmarkArtifacts
  DOTNET_NOLOGO: true
  DOTNET_CLI_TELEMETRY_OPTOUT: true
  VERBOSE: ${{ vars.VERBOSE || false }}

jobs:
  benchmarks:
    name: Benchmarks (${{ inputs.runner-os }})
    permissions:
      checks: write # Required for Bencher to create GitHub Checks
    runs-on: ${{ inputs.runner-os }}
    steps:
      # Install Bencher CLI (install early so if it fails, we fail fast before doing extra work)
      - name: Compute Bencher cache week key
        id: cache-date
        run: echo "week=$(date +%Y-W%V)" >> "$GITHUB_OUTPUT"
        shell: bash

      - name: Cache Bencher CLI
        id: cache-bencher
        uses: actions/cache@v5
        with:
          path: ~/.cargo/bin/bencher
          key: bencher-cli-${{ runner.os }}-${{ steps.cache-date.outputs.week }}
          restore-keys: |
            bencher-cli-${{ runner.os }}-latest-

      - name: Install Bencher CLI
        if: steps.cache-bencher.outputs.cache-hit != 'true'
        uses: bencherdev/bencher@v0.5.10

      # Clone the repository with full Git history
      - name: Checkout
        uses: actions/checkout@v6
        with:
          fetch-depth: 0
          filter: tree:0

      # Clone only the scripts from vm2.DevOps, so that we can run the composite
      # action .github/actions/scripts/action.yaml for both local and external calls
      - name: Checkout vm2.DevOps repository scripts
        if: github.repository != 'vmelamed/vm2.DevOps'
        uses: actions/checkout@v6
        with:
          repository: vmelamed/vm2.DevOps
          path: vm2-devops
          persist-credentials: false
          sparse-checkout: |+
            scripts/bash/lib
            .github/actions/scripts
      - name: Add vm2.DevOps to PATH for work inside vm2.DevOps
        if: github.repository == 'vmelamed/vm2.DevOps'
        uses: ./.github/actions/scripts
      - name: Add vm2.DevOps to PATH for work outside vm2.DevOps
        if: github.repository != 'vmelamed/vm2.DevOps'
        uses: ./vm2-devops/.github/actions/scripts

      # Generate a weekly cache key to automatically rotate NuGet cache
      - name: Get cache timestamp (weekly rotation)
        id: cache-timestamp
        shell: bash
        run: |
          CACHE_WEEK=$(date +%Y-W%V)
          echo "week=$CACHE_WEEK" >> $GITHUB_OUTPUT
          echo "Cache rotation key: $CACHE_WEEK"

      # Install .NET SDK with built-in NuGet package caching
      - name: Setup .NET
        uses: actions/setup-dotnet@v5
        with:
          dotnet-version: ${{ inputs.dotnet-version }}
          dotnet-quality: 'ga'
          cache: true
          cache-dependency-path: |
            **/packages.lock.json
            **/*.csproj

      # Additional explicit cache layer with time-based rotation
      - name: Cache NuGet packages (weekly rotation)
        uses: actions/cache@v5
        with:
          path: ~/.nuget/packages
          key: nuget-${{ runner.os }}-${{ steps.cache-timestamp.outputs.week }}-${{ hashFiles('**/packages.lock.json') }}
          restore-keys: |
            nuget-${{ runner.os }}-${{ steps.cache-timestamp.outputs.week }}-
            nuget-${{ runner.os }}-

      # Retrieve compiled binaries from the build job
      - name: Restore build artifacts from cache
        uses: actions/cache/restore@v5
        with:
          key: build-artifacts-${{ runner.os }}-${{ github.sha }}-${{ inputs.configuration }}-${{ github.run_id }}
          fail-on-cache-miss: true
          path: |
            **/bin/${{ inputs.configuration }}
            **/obj

      # Run benchmarks using BenchmarkDotNet and export JSON results
      - name: Run BenchmarkDotNet benchmarks
        id: run-benchmarks
        shell: bash
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}
          GITHUB_TOKEN: ${{ github.token }}
        run: |
          echo "$DEVOPS_LIB_DIR/gh_core.sh"
          source "$DEVOPS_LIB_DIR/gh_core.sh"

          run-benchmarks.sh ${{ inputs.benchmark-project }} \
              --configuration "${{ inputs.configuration }}" \
              --define "${{ inputs.preprocessor-symbols }}" \
              --minver-tag-prefix "${{ inputs.minver-tag-prefix }}" \
              --minver-prerelease-id "${{ inputs.minver-prerelease-id }}" \
              --artifacts "${{ env.ARTIFACTS_DIR }}"

          # Calculate thresholds as decimals (e.g., 10% -> LOWER_THRESHOLD=0.90, 10% -> UPPER_THRESHOLD=0.10)
          LOWER_THRESHOLD=$(echo "scale=4; 1 - ${{ inputs.max-regression-pct }} / 100" | bc)
          UPPER_THRESHOLD=$(echo "scale=4; ${{ inputs.max-regression-pct }} / 100" | bc)

          # Compute Bencher project slug from repository name, e.g., vmelamed/vm2.DevOps -> vm2-devops
          REPO_NAME="${{ github.repository }}"
          BENCHER_SLUG=$(echo "${REPO_NAME#*/}" | tr '[:upper:]' '[:lower:]' | tr '.' '-')

          # Build bencher run command
          BENCHER_CMD="bencher run                        \
              --github-actions \"$GITHUB_TOKEN\"          \
              --ci-event \"${{ github.event_name }}\"     \
              --ci-number $GITHUB_RUN_NUMBER              \
              --iter 1                                    \
              --fold median                               \
              --project \"${BENCHER_SLUG}\"               \
              --branch \"${{ github.ref_name }}\"         \
              --testbed \"${{ inputs.runner-os }}\"       \
              --threshold-measure latency                 \
              --threshold-test percentage                 \
              --threshold-max-sample-size 64              \
              --threshold-lower-boundary _                \
              --threshold-upper-boundary $UPPER_THRESHOLD \
              --threshold-measure throughput              \
              --threshold-test percentage                 \
              --threshold-max-sample-size 64              \
              --threshold-lower-boundary $LOWER_THRESHOLD \
              --threshold-upper-boundary _                \
              --thresholds-reset"                         \
              --err                                       \
              --adapter c_sharp_dot_net                   \
              --token \"$BENCHER_API_TOKEN\""

          # Add all benchmark result files
          for file in ${{ env.ARTIFACTS_DIR }}/results/*-report.json; do
              if [[ -s "$file" ]]; then
                  trace "Adding benchmark result file: $file"
                  BENCHER_CMD="$BENCHER_CMD --file \"$file\""
              fi
          done

          trace "Will run: ${BENCHER_CMD}"
          eval $BENCHER_CMD

      # Upload benchmark results as workflow artifacts for manual inspection
      - name: Upload benchmark results artifact
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: benchmark-results-${{ inputs.runner-os }}
          path: ${{ env.ARTIFACTS_DIR }}
          if-no-files-found: warn
          overwrite: true

      # Add summary to job
      - name: Add Bencher results to summary
        if: always()
        shell: bash
        run: |
          REPO_NAME="${{ github.repository }}"
          BENCHER_SLUG=$(echo "${REPO_NAME#*/}" | tr '[:upper:]' '[:lower:]' | tr '.' '-')

          {
              echo "### ðŸ“Š Benchmark Results"
              echo ""
              echo "Project: **${{ github.repository }}**"
              echo "Branch: **${{ github.ref_name }}**"
              echo "Testbed: **${{ inputs.runner-os }}**"
              echo "Threshold: **Â±${{ inputs.max-regression-pct }}%**"
              echo ""
              echo "ðŸŒ [View detailed results on Bencher](https://bencher.dev/console/projects/${BENCHER_SLUG})"
          } >> $GITHUB_STEP_SUMMARY
