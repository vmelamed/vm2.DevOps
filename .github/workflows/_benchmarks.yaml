name: _benchmarks (Reusable workflow)

on:
  workflow_call:
    inputs:
      os:
        description: Runner OS-s (cannot be null or empty string, default ubuntu-latest)
        type: string
        default: ubuntu-latest

      dotnet-version:
        description: Version of .NET SDK to use (cannot be null or empty string, default 10.0.x)
        type: string
        default: 10.0.x

      configuration:
        description: The type of build to produce, e.g. Release vs Debug (cannot be null or empty string, default Release)
        type: string
        default: Release

      preprocessor-symbols:
        description: Preprocessor symbols to pass to the compiler
        type: string
        default: "_"

      benchmark-project:
        description: Path to the benchmark project to run benchmarks against (cannot be null or empty string)
        type: string
        required: true

      max-regression-pct:
        description: Maximum acceptable performance regression percentage
        type: number
        default: 10 # percent

      force-new-baseline:
        description: Ignore the existing baseline and force a new baseline
        type: boolean
        default: false

      cached-dependencies:
        description: Enable caching of NuGet dependencies (default true)
        type: boolean
        default: true

      cached-artifacts:
        description: Enable caching of build artifacts (default true)
        type: boolean
        default: true

      verbose:
        description: Whether to enable verbose logging for debugging scripts
        type: boolean
        default: false

# concurrency:
#   group: test-${{ github.workflow_ref }}-${{ inputs.test-project }}
#   cancel-in-progress: true
#   # NOTE: Only uncomment if this workflow is called directly (not via _ci.yaml)
#   # or if _ci.yaml runs build/test/benchmarks in parallel

permissions:
  contents: read

env:
  # these must be the same across all jobs and steps below.
  # We must know where to find the artifacts and where to download/upload to/from across all jobs and steps here.
  ARTIFACTS_DIR: ${{ github.workspace }}/BmArtifacts
  DOTNET_NOLOGO: true
  DOTNET_CLI_TELEMETRY_OPTOUT: true

jobs:
  # Execute performance benchmarks and compare results against baseline
  benchmarks:
    name: Benchmarks (${{ inputs.os }})
    runs-on: ${{ inputs.os }}
    steps:
      # Clone the repository with full Git history
      - name: Checkout
        uses: actions/checkout@v6
        with:
          fetch-depth: 0

      # Generate a weekly cache key to automatically rotate NuGet cache
      - name: Get cache timestamp (weekly rotation)
        if: inputs.cached-dependencies == true
        id: cache-timestamp
        shell: bash
        run: |
          # Generate cache key based on calendar week
          CACHE_WEEK=$(date +%Y-W%V)
          echo "week=$CACHE_WEEK" >> $GITHUB_OUTPUT
          echo "Cache rotation key: $CACHE_WEEK"

      # Install .NET SDK with built-in NuGet package caching
      - name: Setup .NET
        uses: actions/setup-dotnet@v5
        with:
          dotnet-version: ${{ inputs.dotnet-version }}
          cache: ${{ inputs.cached-dependencies }}
          cache-dependency-path: |+
            **/packages.lock.json
            **/*.csproj

      # Additional explicit cache layer with time-based rotation for NuGet packages
      - name: Cache NuGet packages (weekly rotation)
        if: inputs.cached-dependencies == true
        uses: actions/cache@v5
        with:
          path: ~/.nuget/packages
          key: nuget-${{ runner.os }}-${{ steps.cache-timestamp.outputs.week }}-${{ hashFiles('**/packages.lock.json') }}
          restore-keys: |
            nuget-${{ runner.os }}-${{ steps.cache-timestamp.outputs.week }}-
            nuget-${{ runner.os }}-

      # Restore NuGet dependencies only if not using cached dependencies
      - name: Restore dependencies
        if: inputs.cached-dependencies == false
        shell: bash
        run: |
          dotnet restore --locked-mode

      # Retrieve compiled binaries from the build job to avoid recompilation
      - name: Restore build artifacts from cache
        if: inputs.cached-artifacts == true
        uses: actions/cache/restore@v5
        with:
          path: |
            **/bin/${{ inputs.configuration }}
            **/obj
          key: build-artifacts-${{ runner.os }}-${{ github.sha }}-${{ inputs.configuration }}
          fail-on-cache-miss: true

      # Make bash scripts executable on Unix-like systems
      - name: Make Scripts Executable
        shell: bash
        run: |
          chmod u+x ./scripts/bash/*.sh

      # Download previous benchmark results to use as performance baseline (unless forcing new baseline)
      - name: Download baseline benchmark summaries artifact
        if: ${{ !inputs.force-new-baseline }}
        shell: bash
        env:
          GH_TOKEN: ${{ github.token }}
          VERBOSE: ${{ fromJSON(inputs.verbose) || false }}
        run: |
          ./scripts/bash/download-artifact.sh \
              --repository ${{ github.repository }} \
              --wf-path '.github/workflows/Glob.Api.CI.yaml' \
              --artifact benchmark-summaries-${{ inputs.os }} \
              --directory ${{ env.ARTIFACTS_DIR }}/baseline
          # TODO: try to change to something that is not bound by repo name and path of the top level workflow!!!

      - name: Run the benchmarks
        if: ${{ success() || fromJSON(inputs.force-new-baseline) }}
        shell: bash
        env:
          CONFIGURATION: ${{ inputs.configuration }}
          PREPROCESSOR_SYMBOLS: ${{ inputs.preprocessor-symbols }}
          VERBOSE: ${{ fromJSON(inputs.verbose) }}
          FORCE_NEW_BASELINE: ${{ fromJSON(inputs.force-new-baseline) }}
        run: |
          ./scripts/bash/run-benchmarks.sh ${{ inputs.benchmark-project }} \
              --configuration "${{ inputs.configuration }}" \
              --artifacts "${{ env.ARTIFACTS_DIR }}" \
              --max-regression-pct "${{ inputs.max-regression-pct }}" \
              ${{ inputs.cached-dependencies && '--cached-dependencies' || '' }} \
              ${{ inputs.cached-artifacts    && '--cached-artifacts'    || '' }}

      # Upload benchmark results as workflow artifacts for future baseline comparisons
      - name: Upload benchmark summaries artifact
        uses: actions/upload-artifact@v6
        with:
          name: benchmark-summaries-${{ inputs.os }}
          path: ${{ env.ARTIFACTS_DIR }}
          if-no-files-found: error
          overwrite: true

      # If forcing new baseline, save these results as the new baseline
      - name: Upload benchmark results artifact as a new baseline
        if: ${{ inputs.force-new-baseline || false }}
        uses: actions/upload-artifact@v6
        with:
          name: benchmark-baseline-${{ inputs.os }}
          path: ${{ env.ARTIFACTS_DIR }}
          if-no-files-found: error
          overwrite: true
